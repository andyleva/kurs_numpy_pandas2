{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba0d3c5",
   "metadata": {},
   "source": [
    "### Домашнее задание по\n",
    "#### Урок 1. Введение в задачу классификации. Постановка задачи и подготовка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a4e46",
   "metadata": {},
   "source": [
    "### 1) Приведите по 2 примера, когда лучше максимизировать Precision, а когда Recall.\n",
    "1. Уменьшение спама - макс Precission.\n",
    "2. Если кредитная политика банка направлена на большую выручку, имеет смысл максимизировать Recall.\n",
    "3. Если кредитная политика банка направлена на репутацию (макс. клиентов платежеспособны), имеет смысл максимизировать Precision.\n",
    "4. Если при раке речь идет о жизни человека, то количество удаляемого имеет смысл определять исходя из максимизации Recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e5bef",
   "metadata": {},
   "source": [
    "### 2) Почему мы используем F-меру, почему, например, нельзя просто взять среднее от Precision и Recall?\n",
    "F-мера более точно отражает экстремальные значения. второе - Precision и Recall отражают разные процессы (разные величины количеств) и надо приводить их к одному знаменателю и взвешивать по отношению к чему то. взвешивая эти параметры когда\n",
    " хотитне хотим выбирать один или другой параметр для оценки производительности модели. (Если вы считаете Recall более важным, чем Precision, то вам также следует присвоить ему больший вес в среднем расчете (например, F2), и наоборот (например, F0.5).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25658288",
   "metadata": {},
   "source": [
    "### 3)*Реализовать функции для подсчета Accuracy, Precision, Recall, F-score, которые на вход принимают \n",
    "y_true (истинные значения), y_pred (предсказанные значения), а на выход дается метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eed7c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 1, 2, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de8e46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrica(y_true, y_pred, binary_str = None):\n",
    "    #binary_str = 'micro', 'macro', 'weighted', None\n",
    "    from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)\n",
    "    accuracy_metrica = accuracy_score(y_true, y_pred)\n",
    "    print(f\"accuracy = {accuracy_metrica}\")\n",
    "    precision_metrica = precision_score(y_true, y_pred, average = binary_str)\n",
    "    print(f\"precision = {precision_metrica}\")\n",
    "    recall_metrica = recall_score(y_true, y_pred, average = binary_str)\n",
    "    print(f\"recall = {recall_metrica}\")\n",
    "    f1_score_metrica = f1_score(y_true, y_pred, average = binary_str)\n",
    "    print(f\"f1_score = {f1_score_metrica}\")\n",
    "    return accuracy_metrica, precision_metrica, recall_metrica, f1_score_metrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "418e2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8333333333333334\n",
      "precision = 0.8888888888888888\n",
      "recall = 0.8333333333333334\n",
      "f1_score = 0.8222222222222223\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = all_metrica(y_true, y_pred,'macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
